{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21aaf3ea-5e47-44f5-9885-dc73a12e4216",
   "metadata": {},
   "source": [
    "# ETL Master Flight Table Using PySpark\n",
    "The master flight table combines attributes from external tables to form a single flat table to use as a starting point for training our ML models. External attribute tables such as airport demand and weather can be pre-partitioned by airport name. External tables are typically joined to the master flight table twice: once for arrivals and a second time for departures. Data alignment is required to merge these tables so multiple shuffle operations are required for each of the joins. Pre-partitioning the data ensures that the dominant join pattern can take advantage of less expensive, in-block operations to avoid unnecessary data movement. This becomes critical for large datasets in a distributed cluster environment.  \n",
    "\n",
    "Hand tuning data flows requires a deep understanding of internal data structures. Operations that worked for small data samples may not be scalable to the full dataset. This provides some motivation to try to improve performance using advanced libraries such as spark, dask, or NVTabular to enable us to reuse simple code and scale it to the full dataset without having to resort to writing additional custom code or patches for resolving performance issues at scale. With the RAPIDS plugin for spark, the same code used on CPU deployments can be reused on the GPU to accelerate the ETL pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04872737-489c-4527-87de-9e6b0e81299d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/28 18:30:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/01/28 18:30:08 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://boxx-wlo2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[32]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1e9b88bac0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "# Load external tables for merging:\n",
    "staging_dir = './data/staging_tbl/'\n",
    "etl_output_dir = './data/encoded/NAS/' # Directory will be wiped!\n",
    "os.makedirs(etl_output_dir, exist_ok = True)\n",
    "\n",
    "try:\n",
    "    # RECURSIVELY DELETE DIRECTORY and then add it\n",
    "    shutil.rmtree(etl_output_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "os.mkdir(etl_output_dir)\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark_cluster = 'local'\n",
    "\n",
    "if spark_cluster == 'local':\n",
    "    num_exec = 32\n",
    "    spark = (pyspark.sql.SparkSession.builder\n",
    "             .master('local['+str(num_exec)+']')\n",
    "             .config('spark.executor.memory', '50g')\n",
    "             .config('spark.driver.memory', '150g')\n",
    "             .config('spark.sql.adaptive.enable', 'true')\n",
    "             .config('spark.sql.shuffle.partitions', 10*num_exec)\n",
    "             .config('spark.sql.execution.arrow.pyspark.enabled', 'true')\n",
    "             .config('spark.local.dir', './tmp/spark_tmp/') # Optional. Main drive was filling up.\n",
    "            ).getOrCreate()\n",
    "else:\n",
    "    spark_master = 'spark://boxx-wlo2:7077' # Cluster\n",
    "    spark = (pyspark.sql.SparkSession.builder\n",
    "             .master(spark_master)\n",
    "             .config('spark.sql.shuffle.partitions', 32)\n",
    "             .config('spark.sql.execution.arrow.pyspark.enabled', 'true')\n",
    "             .config('spark.local.dir', './tmp/spark_tmp/')  # Optional. Main drive was filling up.\n",
    "            ).getOrCreate()\n",
    "\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd6d04-b22f-4f51-a8aa-8ae0ae9bec31",
   "metadata": {},
   "source": [
    "# Spark Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b195b74a-8673-4c38-9804-f897e23e9a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/28 18:30:12 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airport demand shape: (1,989,473 34)\n",
      "Airport weather shape: (568,916 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==================================================>     (20 + 2) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flight table shape: (1,555,982 61)\n",
      "CPU times: user 26.7 ms, sys: 7.79 ms, total: 34.5 ms\n",
      "Wall time: 11.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def spark_shape(df):\n",
    "    return('(' + f'{df.count():,}' + ' ' +str(len(df.columns)) +')' )\n",
    "\n",
    "drop_meta_cols = ['__null_dask_index__', '__index_level_0__']\n",
    "\n",
    "arpt_demand = spark.read.parquet(staging_dir+'arpt_demand').drop(*drop_meta_cols, 'YEAR').cache()\n",
    "print('Airport demand shape:', spark_shape(arpt_demand))\n",
    "\n",
    "arpt_weather = spark.read.parquet(staging_dir+'arpt_weather').drop(*drop_meta_cols, 'YEAR').cache()\n",
    "print('Airport weather shape:', spark_shape(arpt_weather))\n",
    "\n",
    "nas_flights = spark.read.parquet(staging_dir+'nas_flights').drop(*drop_meta_cols).cache()\n",
    "print('Flight table shape:', spark_shape(nas_flights))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7d76c65-af40-4292-a895-a46b88181d57",
   "metadata": {},
   "source": [
    "# Check uniqueness of weather data:\n",
    "arpt_weather.groupby(['ARPT_NAME', 'DT_LOCAL_HR']).count().sort(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ece6f30-a43e-4979-97d6-1ef352e797fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.2 ms, sys: 0 ns, total: 48.2 ms\n",
      "Wall time: 19.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def add_col_prefix(df, update_col_names, prefix, ignore_cols):\n",
    "    cols_with_prefix = [cc for cc in update_col_names if cc not in ignore_cols]\n",
    "    col_rename_map = list(zip(cols_with_prefix, [prefix+cc for cc in cols_with_prefix]))\n",
    "    \n",
    "    for cc in col_rename_map:\n",
    "        df = df.withColumnRenamed(*cc)\n",
    "    return(df)\n",
    "\n",
    "# Create master flight table by combining attributes from ORIGIN/DEST airports.\n",
    "# Merge and rename new columns afterward involves 3 table scan instead of 5. External tables can be cached for ease of reuse.\n",
    "\n",
    "#####################################\n",
    "## Merge Airport Demand Attributes ##\n",
    "#####################################\n",
    "# ORIGIN airport demand:\n",
    "nas_flights_mg = (nas_flights.join(arpt_demand, \n",
    "                                  (nas_flights['ORIGIN']==arpt_demand['ARPT_NAME']) & \n",
    "                                  (nas_flights['DEP_TIME_DT_LOCAL_QTHR']==arpt_demand['DT_LOCAL_QTHR']), 'left')\n",
    "                  .drop('ARPT_NAME', 'DT_LOCAL_QTHR')\n",
    "                 )\n",
    "nas_flights_mg = add_col_prefix(nas_flights_mg, arpt_demand.columns, 'ORIGIN_', ['ARPT_NAME', 'DT_LOCAL_QTHR'])\n",
    "\n",
    "# DEST airport demand:\n",
    "nas_flights_mg = (nas_flights_mg.join(arpt_demand, \n",
    "                                  (nas_flights_mg['DEST']==arpt_demand['ARPT_NAME']) & \n",
    "                                  (nas_flights_mg['ARR_TIME_DT_LOCAL_QTHR']==arpt_demand['DT_LOCAL_QTHR']), 'left')\n",
    "                  .drop('ARPT_NAME', 'DT_LOCAL_QTHR')\n",
    "                 )\n",
    "nas_flights_mg = add_col_prefix(nas_flights_mg, arpt_demand.columns, 'DEST_', ['ARPT_NAME', 'DT_LOCAL_QTHR'])\n",
    "\n",
    "\n",
    "######################################\n",
    "## Merge Airport Weather Attributes ##\n",
    "######################################\n",
    "# ORIGIN airport weather:\n",
    "nas_flights_mg = (nas_flights_mg.join(arpt_weather, \n",
    "                                      (nas_flights_mg['ORIGIN']==arpt_weather['ARPT_NAME']) &\n",
    "                                      (nas_flights_mg['DEP_TIME_DT_LOCAL_HR']==arpt_weather['DT_LOCAL_HR']), 'left')\n",
    "                  .drop('ARPT_NAME', 'DT_LOCAL_HR')\n",
    "                 )\n",
    "nas_flights_mg = add_col_prefix(nas_flights_mg, arpt_weather.columns, 'ORIGIN_', ['ARPT_NAME', 'DT_LOCAL_HR'])\n",
    "\n",
    "# DEST airport weather:\n",
    "nas_flights_mg = (nas_flights_mg.join(arpt_weather, \n",
    "                                      (nas_flights_mg['DEST']==arpt_weather['ARPT_NAME']) &\n",
    "                                      (nas_flights_mg['ARR_TIME_DT_LOCAL_HR']==arpt_weather['DT_LOCAL_HR']), 'left')\n",
    "                  .drop('ARPT_NAME', 'DT_LOCAL_HR')\n",
    "                 )\n",
    "nas_flights_mg = add_col_prefix(nas_flights_mg, arpt_weather.columns, 'DEST_', ['ARPT_NAME', 'DT_LOCAL_HR'])\n",
    "\n",
    "###################\n",
    "## Clean Columns ##\n",
    "###################\n",
    "arpt_demand_attr_cols = [cc for cc in arpt_demand.columns if cc not in ['YEAR', 'ARPT_NAME', 'DT_LOCAL_QTHR']]\n",
    "arpt_demand_attr_cols = [prefix + dcols for prefix in ['ORIGIN_', 'DEST_'] for dcols in arpt_demand_attr_cols]\n",
    "\n",
    "# Fill missing data due to merging:\n",
    "nas_flights_mg = nas_flights_mg.fillna(0, subset=arpt_demand_attr_cols)\n",
    "\n",
    "# Due to sporadic weather data, missing fields can only be dropped instead of imputed.\n",
    "\n",
    "# Remove certain cols with ORIGIN_/DEST_ prefix:\n",
    "od_remove_cols = ['DT_LOCAL_QTHR', 'DT_LOCAL_HR']\n",
    "od_remove_cols = [pfix+cc for pfix in ['ORIGIN_', 'DEST_'] for cc in od_remove_cols]\n",
    "\n",
    "# Remove certain cols with ARR/DEP_ prefix:\n",
    "ad_remove_cols = ['TIME_DT_LOCAL', 'TIME_DT_LOCAL_DAY', 'TIME_DT_LOCAL_HR', 'TIME_DT_LOCAL_QTHR']\n",
    "ad_remove_cols = [pfix+cc for pfix in ['ARR_', 'DEP_'] for cc in ad_remove_cols]\n",
    "\n",
    "# Drop unecessary columns to prevent leakage and duplicate data:\n",
    "nas_flights_mg = nas_flights_mg.drop(*(ad_remove_cols + od_remove_cols))\n",
    "\n",
    "# Using coalesce(1) results in single task and reduced parallelism. repartition() involves shuffling, but is parallel.\n",
    "# Not applying coalesce/repartion results in large number of small files (equal to the number of spark.sql.shuffle.partitions).\n",
    "nas_flights_mg.coalesce(4).write.mode('overwrite').partitionBy('YYYYMM').parquet(etl_output_dir)\n",
    "\n",
    "# TODO: optimize merging.\n",
    "# Chain merge of 17-year data took around 17 minutes on 2x rtx8000 vs 35 minutes for 32-core CPU run.\n",
    "# Chain merge of 32-year data took around xx minutes on 2x rtx8000 vs 96 minutes for 32-core CPU run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
