{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7003c99-735c-4b6e-8fd7-2deb193ac365",
   "metadata": {},
   "source": [
    "# Test Train Split\n",
    "Target encoding is applied to several categorical features. To prevent leakage, target encoding needs to be applied after splitting the data into test/train sets. The python \"category_encoders\" package uses pandas to perform target encoding on a single CPU-core. Due to memory limitations, the following implementation slices UID, label column, and target encode columns from the master flight table to perform the encoding against the test/train set independently. Post-processing is applied to merge and prepare the fully encoded tables for XGBOOST. \n",
    "\n",
    "Time series modeling requires that the data be split into continuous chunks to avoid potential leakage of information between neighboring events. The data is split into continuous chunks with the following date ranges: 200306 to 201903 train; 201904 to 202003 test. Due to COVID-19, data after 202003 is omitted since the model may not have enough data to resolve the irregularities in flight demand and schedules. A large number of flights were cancelled during this time period, which may not be adequately represented since cancelled/diverted flights were removed from consideration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e1aa03-151f-4f9a-84a2-52061077a4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost version 1.5.0-dev\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:38999</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>30</li>\n",
       "  <li><b>Cores: </b>30</li>\n",
       "  <li><b>Memory: </b>251.65 GiB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:38999' processes=30 threads=30, memory=251.65 GiB>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import numpy as np\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, wait, progress, get_worker\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost.dask import DaskDMatrix\n",
    "\n",
    "print('xgboost version', xgb.__version__)\n",
    "\n",
    "\n",
    "study_arpt = 'NAS'\n",
    "run_type = 'cpu'\n",
    "storage_backend = 'local'\n",
    "\n",
    "num_folds = 8 # Choose sensible number that's a multiple of number of nodes in cluster to avoid stragglers.\n",
    "num_holdout_months = 12 # Number of months to reserve for holdout test. Removed from end of time period of interest.\n",
    "apply_tgt_enc = True # Run target encoder on partially encoded data.\n",
    "\n",
    "if study_arpt == 'NAS':\n",
    "    # NAS processing excludes weather. Has additional cols for arrival/departure airports.\n",
    "    pred_model = 'multi_class'\n",
    "    label_col = 'DELAY_CAUSES_ENC'\n",
    "    delay_causes_cols = ['LATE_AIRCRAFT_DELAY', 'CARRIER_DELAY', 'NAS_DELAY', 'OTHER_DELAY']\n",
    "    excluded_features = [label_col, 'cv_idx', 'UID', 'ARR_DEL15', 'DEP_DEL15']\n",
    "    target_encode_cols = ['OP_UNIQUE_CARRIER', 'ORIGIN', 'DEST', 'OD_PAIR', 'HOLIDAY_NAME', 'TAIL_NUM', \n",
    "                          'ORIGIN_HourlyPresentWeatherTypeCombo', 'DEST_HourlyPresentWeatherTypeCombo']\n",
    "else:\n",
    "    pred_model = 'binary_class'\n",
    "    label_col = 'ARR_DEL15'\n",
    "    excluded_features = [label_col, 'cv_idx', 'UID', 'DEL_ARR_PER_QTHR', 'DEL_DEP_PER_QTHR', 'DEP_DEL15', 'ARR_DEL']\n",
    "    target_encode_cols = ['OP_UNIQUE_CARRIER', 'ORIGIN', 'HOLIDAY_NAME', 'HourlyPresentWeatherTypeCombo', 'TAIL_NUM']\n",
    "\n",
    "if pred_model == 'binary_class':\n",
    "    xgb_objective = 'binary:logistic'\n",
    "elif pred_model == 'multi_class':\n",
    "    # xgboost auc docs mentioned that: \"When used with multi-class classification, objective should be multi:softprob instead of multi:softmax, \n",
    "    # as the latter doesn’t output probability. Also the AUC is calculated by 1-vs-rest with reference class weighted by class prevalence.\"\n",
    "    xgb_objective = 'multi:softprob'\n",
    "# elif pred_model == 'regression':\n",
    "#     label_col = 'ARR_DELAY' # Regression model\n",
    "#     xgb_objective = 'reg:squarederror'\n",
    "    \n",
    "partial_enc_input_dir = './data/encoded/'+study_arpt # Partially encoded data. Target encoding required.\n",
    "enc_output_dir = './data/staging_tbl/split_target_enc/'+study_arpt # DIRECTORY WILL BE WIPED EACH RUN. Fully encoded data with test/train subfolders.\n",
    "fully_enc_output_dir = './data/encoded/split/'+study_arpt # DIRECTORY WILL BE WIPED EACH RUN. \n",
    "\n",
    "xgb_model_name = 'xgb_'+run_type+'_airline_delay_'+study_arpt\n",
    "\n",
    "if run_type == 'gpu':\n",
    "    client = Client(n_workers=1, threads_per_worker=16)\n",
    "elif run_type == 'cpu':\n",
    "    client = Client(n_workers=30, threads_per_worker=1)\n",
    "#     client = Client('tcp://192.168.1.232:8785')\n",
    "    \n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c42b8846-72ec-4e63-a206-6947d98cdf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set_name\n",
      "impute_past       [200306, 200307, 200308, 200309, 200310, 20031...\n",
      "train             [200906, 200907, 200908, 200909, 200910, 20091...\n",
      "test              [201904, 201905, 201906, 201907, 201908, 20190...\n",
      "predict_future    [202004, 202005, 202006, 202007, 202008, 20200...\n",
      "Name: DATE, dtype: object\n",
      "\n",
      "CPU times: user 265 ms, sys: 74.7 ms, total: 340 ms\n",
      "Wall time: 3.25 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>DATE</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>set_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/encoded/NAS/YYYYMM=198710/part-00000-52...</td>\n",
       "      <td>198710</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/encoded/NAS/YYYYMM=198710/part-00001-52...</td>\n",
       "      <td>198710</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/encoded/NAS/YYYYMM=198710/part-00002-52...</td>\n",
       "      <td>198710</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/encoded/NAS/YYYYMM=198710/part-00003-52...</td>\n",
       "      <td>198710</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/encoded/NAS/YYYYMM=198711/part-00000-52...</td>\n",
       "      <td>198711</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>./data/encoded/NAS/YYYYMM=202103/part-00003-52...</td>\n",
       "      <td>202103</td>\n",
       "      <td>3</td>\n",
       "      <td>predict_future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>./data/encoded/NAS/YYYYMM=202104/part-00000-52...</td>\n",
       "      <td>202104</td>\n",
       "      <td>4</td>\n",
       "      <td>predict_future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>./data/encoded/NAS/YYYYMM=202104/part-00001-52...</td>\n",
       "      <td>202104</td>\n",
       "      <td>4</td>\n",
       "      <td>predict_future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>./data/encoded/NAS/YYYYMM=202104/part-00002-52...</td>\n",
       "      <td>202104</td>\n",
       "      <td>4</td>\n",
       "      <td>predict_future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>./data/encoded/NAS/YYYYMM=202104/part-00003-52...</td>\n",
       "      <td>202104</td>\n",
       "      <td>4</td>\n",
       "      <td>predict_future</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1612 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               filename    DATE  MONTH  \\\n",
       "0     ./data/encoded/NAS/YYYYMM=198710/part-00000-52...  198710     10   \n",
       "1     ./data/encoded/NAS/YYYYMM=198710/part-00001-52...  198710     10   \n",
       "2     ./data/encoded/NAS/YYYYMM=198710/part-00002-52...  198710     10   \n",
       "3     ./data/encoded/NAS/YYYYMM=198710/part-00003-52...  198710     10   \n",
       "4     ./data/encoded/NAS/YYYYMM=198711/part-00000-52...  198711     11   \n",
       "...                                                 ...     ...    ...   \n",
       "1607  ./data/encoded/NAS/YYYYMM=202103/part-00003-52...  202103      3   \n",
       "1608  ./data/encoded/NAS/YYYYMM=202104/part-00000-52...  202104      4   \n",
       "1609  ./data/encoded/NAS/YYYYMM=202104/part-00001-52...  202104      4   \n",
       "1610  ./data/encoded/NAS/YYYYMM=202104/part-00002-52...  202104      4   \n",
       "1611  ./data/encoded/NAS/YYYYMM=202104/part-00003-52...  202104      4   \n",
       "\n",
       "            set_name  \n",
       "0                NaN  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  \n",
       "...              ...  \n",
       "1607  predict_future  \n",
       "1608  predict_future  \n",
       "1609  predict_future  \n",
       "1610  predict_future  \n",
       "1611  predict_future  \n",
       "\n",
       "[1612 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def init_output_dirs(output_dir, subdirectory):\n",
    "    \"\"\"\n",
    "    Create or wipe existing directory for outputs. Directory will be wiped.\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "\n",
    "    try:\n",
    "        # RECURSIVELY DELETE DIRECTORY and then add it\n",
    "        shutil.rmtree(output_dir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for sd in subdirectory:\n",
    "        Path(output_dir+'/'+sd).mkdir(parents=True)\n",
    "        \n",
    "    print(output_dir + ' has been initialized.')\n",
    "    return()\n",
    "\n",
    "def get_data_split_files(toc, set_name):\n",
    "    \"\"\"\n",
    "    Obtain list of files from table of content. \n",
    "    \"\"\"\n",
    "    # Train set is cv_idx!=-1. Only need to apply target encoder to results since remaining data pre-encoded.\n",
    "    if set_name == 'test':\n",
    "        out = list(toc[toc['cv_idx'] == -1]['filename'])\n",
    "    elif set_name == 'train':\n",
    "        out = list(toc[toc['cv_idx'] != -1]['filename'])\n",
    "    elif set_name == 'cv':\n",
    "        # TODO: Return list of list containing the folds.\n",
    "        out = []\n",
    "    else:\n",
    "        raise ValueError('Case not implemented. Choose test, train, or cv.')\n",
    "    return(out)\n",
    "\n",
    "\n",
    "pq_files = glob.glob(partial_enc_input_dir+'/*/*.parquet')\n",
    "toc_df = pd.DataFrame({'filename': pq_files})\n",
    "toc_df['DATE'] = toc_df['filename'].str.split('=').str[1].str[:6]\n",
    "toc_df['DATE'] = toc_df['DATE'].astype(int)\n",
    "toc_df['MONTH'] = toc_df['DATE'].astype(str).str[-2:].astype('int8')\n",
    "toc_df = toc_df.sort_values(['DATE', 'filename']).reset_index(drop=True)\n",
    "\n",
    "# Split data into various sets. Test/train sets have known labels.\n",
    "# Filter for specific dates. Exclude months after March 2020 from data due to COVID.\n",
    "# Exclude before 200306 due to lack of delay cause attribution. Treat this as if it were test data.\n",
    "end_date = 202003 # Date when data is not used in test/train set.\n",
    "start_date = pd.to_datetime(end_date, format='%Y%m') - relativedelta(months=num_holdout_months)\n",
    "start_date = int(start_date.strftime('%Y%m'))\n",
    "\n",
    "# split_dates = [198701, 200305, start_date, end_date, 202106] # Full data\n",
    "# split_dates = [201605, 201805, start_date, end_date, 202106] # Truncated data with ~1 years of training, 1 year test.\n",
    "# split_dates = [201205, 201405, start_date, end_date, 202106] # Truncated data with ~5 years of training, 1 year test.\n",
    "split_dates = [200305, 200905, start_date, end_date, 202106] # Truncated data with ~10 years of training, 1 year test.\n",
    "# split_dates = [200305, 200505, start_date, end_date, 202106] # Truncated data with ~14 years of training, 1 year test.\n",
    "split_labels = ['impute_past', 'train', 'test', 'predict_future']\n",
    "\n",
    "toc_df['set_name'] = pd.cut(toc_df['DATE'], bins=split_dates, \n",
    "                            labels=split_labels)\n",
    "\n",
    "# Need train set to be first since target encoder needs to be trained before it can be applied to subsequent data:\n",
    "split_labels_reorder = ['train'] + [cc for cc in split_labels if cc != 'train']\n",
    "\n",
    "print(toc_df.groupby('set_name')['DATE'].unique())\n",
    "print()\n",
    "\n",
    "toc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f713e4-8f6e-4edc-bb51-bc0eea151d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of meta-classes after applying 0.95 delayed fraction threshold: 9\n",
      "CPU times: user 3.8 s, sys: 632 ms, total: 4.43 s\n",
      "Wall time: 20.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>delayed_frac</th>\n",
       "      <th>cumsum</th>\n",
       "      <th>new_int_code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS_STR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>----</th>\n",
       "      <td>172731139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--N-</th>\n",
       "      <td>4780127</td>\n",
       "      <td>0.218095</td>\n",
       "      <td>0.218095</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-C--</th>\n",
       "      <td>3067585</td>\n",
       "      <td>0.139960</td>\n",
       "      <td>0.358055</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LC--</th>\n",
       "      <td>2828109</td>\n",
       "      <td>0.129033</td>\n",
       "      <td>0.487088</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L---</th>\n",
       "      <td>2827236</td>\n",
       "      <td>0.128994</td>\n",
       "      <td>0.616082</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-CN-</th>\n",
       "      <td>2600193</td>\n",
       "      <td>0.118635</td>\n",
       "      <td>0.734716</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L-N-</th>\n",
       "      <td>2547345</td>\n",
       "      <td>0.116223</td>\n",
       "      <td>0.850940</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LCN-</th>\n",
       "      <td>1797222</td>\n",
       "      <td>0.081999</td>\n",
       "      <td>0.932939</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--NO</th>\n",
       "      <td>460206</td>\n",
       "      <td>0.020997</td>\n",
       "      <td>0.953936</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>---O</th>\n",
       "      <td>324927</td>\n",
       "      <td>0.014825</td>\n",
       "      <td>0.968761</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L-NO</th>\n",
       "      <td>306594</td>\n",
       "      <td>0.013988</td>\n",
       "      <td>0.982749</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L--O</th>\n",
       "      <td>296649</td>\n",
       "      <td>0.013535</td>\n",
       "      <td>0.996284</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-CNO</th>\n",
       "      <td>21548</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.997267</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LCNO</th>\n",
       "      <td>21119</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.998230</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-C-O</th>\n",
       "      <td>20914</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.999185</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LC-O</th>\n",
       "      <td>17872</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count  delayed_frac    cumsum  new_int_code\n",
       "CLASS_STR                                                 \n",
       "----       172731139      0.000000  0.000000             0\n",
       "--N-         4780127      0.218095  0.218095             1\n",
       "-C--         3067585      0.139960  0.358055             2\n",
       "LC--         2828109      0.129033  0.487088             3\n",
       "L---         2827236      0.128994  0.616082             4\n",
       "-CN-         2600193      0.118635  0.734716             5\n",
       "L-N-         2547345      0.116223  0.850940             6\n",
       "LCN-         1797222      0.081999  0.932939             7\n",
       "--NO          460206      0.020997  0.953936             8\n",
       "---O          324927      0.014825  0.968761             8\n",
       "L-NO          306594      0.013988  0.982749             8\n",
       "L--O          296649      0.013535  0.996284             8\n",
       "-CNO           21548      0.000983  0.997267             8\n",
       "LCNO           21119      0.000964  0.998230             8\n",
       "-C-O           20914      0.000954  0.999185             8\n",
       "LC-O           17872      0.000815  1.000000             8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if label_col == 'DELAY_CAUSES_ENC':\n",
    "    # Reduce number of meta-classes when processing NAS. 2^4 categories not feasible due to data imbalance. \n",
    "    # Multi-class label_col was converted to bitstring (or bitmask). python built-in bin() funtion can be used to obtain bitstring representation.\n",
    "    meta_class_cnts = dd.read_parquet(partial_enc_input_dir, columns=label_col).value_counts().compute()\n",
    "    meta_class_cnts = pd.DataFrame(meta_class_cnts).reset_index()\n",
    "    meta_class_cnts.columns = ['CLASS_STR', 'count']\n",
    "    meta_class_cnts['CLASS_STR'] = meta_class_cnts['CLASS_STR'].astype(str)\n",
    "    meta_class_cnts = meta_class_cnts.set_index('CLASS_STR')\n",
    "\n",
    "\n",
    "    # Number of classes in original label_col:\n",
    "    num_classes_orig = len(meta_class_cnts)\n",
    "    zero_class_code = '-'*(len(meta_class_cnts.index[0]))\n",
    "\n",
    "    meta_class_cnts['delayed_frac'] = meta_class_cnts['count']/meta_class_cnts.loc[meta_class_cnts.index != zero_class_code]['count'].sum()\n",
    "    meta_class_cnts.loc[zero_class_code, 'delayed_frac'] = 0\n",
    "\n",
    "    meta_class_cnts['cumsum'] = meta_class_cnts['delayed_frac'].cumsum()\n",
    "\n",
    "    # Select cutoff percentile to reduce number of combo-classes. \n",
    "    cumsum_thresh = 0.95\n",
    "\n",
    "    # xgboost multi-class requies sequential int codes in label_col. Otherwise, get the following error:\n",
    "    # \".../src/objective/multiclass_obj.cu:120: SoftmaxMultiClassObj: label must be in [0, num_class)\"\n",
    "    meta_class_cnts['new_int_code'] = np.arange(0, len(meta_class_cnts))\n",
    "    thresh_mask = meta_class_cnts['cumsum'] >= cumsum_thresh\n",
    "\n",
    "    # Map new \"other\" category to int outside of normal range [0,num_classes_orig-1]:\n",
    "    meta_class_cnts.loc[thresh_mask, 'new_int_code'] = np.sum(~thresh_mask)\n",
    "\n",
    "    delay_causes_remap = dict(zip(meta_class_cnts.index, meta_class_cnts['new_int_code']))\n",
    "    num_classes_remap = len(meta_class_cnts['new_int_code'].unique())\n",
    "\n",
    "    print('Number of meta-classes after applying '+str(cumsum_thresh)+' delayed fraction threshold:', num_classes_remap)\n",
    "else:\n",
    "    meta_class_cnts = np.nan\n",
    "    \n",
    "\n",
    "meta_class_cnts.to_csv('./data/staging_tbl/class_labels.csv')\n",
    "meta_class_cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98aa5ac3-fee4-4554-afee-ae1a22e30e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/staging_tbl/split_target_enc/NAS has been initialized.\n",
      "./data/encoded/split/NAS has been initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/btong/miniconda3/envs/rapids-dev/lib/python3.8/site-packages/distributed/node.py:160: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 40949 instead\n",
      "  warnings.warn(\n",
      "/home/btong/miniconda3/envs/rapids-dev/lib/python3.8/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoding completed for ./data/staging_tbl/split_target_enc/NAS/train in 246.09s\n",
      "Target encoding completed for ./data/staging_tbl/split_target_enc/NAS/impute_past in 101.9s\n",
      "Target encoding completed for ./data/staging_tbl/split_target_enc/NAS/test in 16.62s\n",
      "Target encoding completed for ./data/staging_tbl/split_target_enc/NAS/predict_future in 14.87s\n",
      "CPU times: user 4min, sys: 54.7 s, total: 4min 55s\n",
      "Wall time: 6min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def df_to_numeric(ddf):\n",
    "    \"\"\"\n",
    "    Perform data pre-processing to create a fully numeric dataframe.\n",
    "    \"\"\"\n",
    "    ddf['YYYYMM'] = ddf['UID']//10000000\n",
    "\n",
    "    # category_encoders TargetEncoder doesn't understand dask. Need to convert to pandas prior to encoding.\n",
    "    # Similarly, cuML TargetEncoder doesn't understand dask_cudf. Need to convert to cudf prior to encoding.\n",
    "    df = ddf.compute()\n",
    "\n",
    "    # Convert remaining object columns to category:\n",
    "    obj_dtypes = list(df.select_dtypes('object').columns)\n",
    "#     print('Object columns to categorize:', obj_dtypes)\n",
    "    \n",
    "    df[obj_dtypes] = df[obj_dtypes].astype('category')\n",
    "\n",
    "    # Handle categorical features by obtaining category codes. TargetEncoder doesn't currently work with categorical/object dtypes.\n",
    "    cat_dtypes = list(df.select_dtypes('category').columns)\n",
    "#     print('Categorical columns detected: ', cat_dtypes)\n",
    "\n",
    "    for cc in cat_dtypes:\n",
    "        if cc == 'DELAY_CAUSES_ENC':\n",
    "            # Apply remapping based on class prevalence to reduce overall number of classes.\n",
    "            df[cc] = df[cc].map(delay_causes_remap).astype('int8')\n",
    "        else:\n",
    "            df[cc] = df[cc].cat.codes.values\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "# TargetEncoder from category_encoders doesn't understand dask.dataframe. Need to convert to pandas with unique index to merge against.\n",
    "# Also, cuML implementation of TargetEncoder appears to be more complex than the scikit compatible implementation of category_encoders.TargetEncoder(). \n",
    "# The encoded results are expected to be different so selecting one of the methods is required to get consistent encodings.\n",
    "        \n",
    "if apply_tgt_enc == True:\n",
    "    from category_encoders import TargetEncoder # sklearn compatible target encoder\n",
    "    \n",
    "    # Initialize output directories:\n",
    "    init_output_dirs(enc_output_dir, split_labels_reorder)\n",
    "    init_output_dirs(fully_enc_output_dir, split_labels_reorder)\n",
    "    \n",
    "    # Run encoder on local dask cluster:\n",
    "    with Client(n_workers=2, threads_per_worker=4) as client_local:\n",
    "        import dask.dataframe as hw\n",
    "        \n",
    "        # Select small subset of columns to limit memory usage.\n",
    "        sel_cols = ['UID', label_col] + target_encode_cols            \n",
    "        \n",
    "        for set_name in split_labels_reorder:\n",
    "            tic = time()\n",
    "            ddf = hw.read_parquet(list(toc_df[toc_df['set_name']==set_name]['filename'].values), columns=sel_cols)\n",
    "            df = df_to_numeric(ddf)\n",
    "            \n",
    "            if set_name == 'train':                \n",
    "                # Train target encoder on training set only. For other data set, we need to apply the learned target encoding to avoid leakage.\n",
    "                # It is assumed that the other data set are of similar composition for the encoding to be applicable.\n",
    "                if run_type == 'cpu':\n",
    "                    tgt_enc_fit = TargetEncoder(cols=target_encode_cols).fit(df[target_encode_cols], df[label_col])\n",
    "                \n",
    "                # TODO: update and fix gpu implementation....\n",
    "#                 elif run_type == 'gpu':\n",
    "#                     from cuml.preprocessing.TargetEncoder import TargetEncoder\n",
    "#                     import cudf\n",
    "\n",
    "#                     encoder = TargetEncoder(n_folds=5, split_method='continuous', seed=0)\n",
    "#                     tmp_ = df[target_encode_cols]\n",
    "\n",
    "#                     # cuML TargetEncoder only considers single column at a time.\n",
    "#                     # String Arrays is not yet implemented in cudf so can't use .values.\n",
    "#                     # Categorical dtype not implemented in cudf either. Would need to target encode int values?\n",
    "#                     for cc in target_encode_cols:\n",
    "#                         tmp_[cc] = encoder.fit_transform(tmp_[cc], df[label_col])\n",
    "\n",
    "#                     # Doing direct replacement leads to weird behavior in cudf.\n",
    "#                     df[target_encode_cols] = tmp_.values\n",
    "                \n",
    "            # Assume target encoder has been trained. Now apply it using .transform():\n",
    "            df[target_encode_cols] = tgt_enc_fit.transform(df[target_encode_cols])\n",
    "            \n",
    "            # Casting to reduce storage:\n",
    "            df[target_encode_cols] = df[target_encode_cols].astype('float32') # Half-float not supported in RAPIDS version of pyarrow yet.\n",
    "            \n",
    "            # Add weight column for training set:\n",
    "            if set_name == 'train':\n",
    "                from sklearn.utils import compute_sample_weight\n",
    "                \n",
    "                # 'balanced' weight in sklearn.utils.class_weight.compute_sample_weight = n_samples / (n_classes * np.bincount(y))\n",
    "                df['class_weight'] = compute_sample_weight('balanced', df[label_col])\n",
    "                \n",
    "            # Write data to disk using dask to ensure muti-partition output.\n",
    "            dd.from_pandas(df, npartitions=1).to_parquet(enc_output_dir+'/'+set_name, write_metadata_file=False, partition_on=['YYYYMM'], flavor='spark')\n",
    "            \n",
    "            toc = np.round(time() - tic, 2)\n",
    "            print('Target encoding completed for ' + enc_output_dir+'/'+set_name +' in '+str(toc)+'s')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54e321c1-01ce-4e12-8df9-39ebbd3e4eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/encoded/split/NAS has been initialized.\n",
      "Merging TargetEncoder results with master flight table.\n",
      "Processing train set...\n",
      "  took 93.32s.\n",
      "\n",
      "Processing impute_past set...\n",
      "  took 89.29s.\n",
      "\n",
      "Processing test set...\n",
      "  took 11.84s.\n",
      "\n",
      "Processing predict_future set...\n",
      "  took 6.82s.\n",
      "\n",
      "CPU times: user 20.3 s, sys: 3.77 s, total: 24.1 s\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize output directories:\n",
    "init_output_dirs(fully_enc_output_dir, split_labels_reorder)\n",
    "\n",
    "@dask.delayed\n",
    "def merge_uid_by_month(set_name, month):\n",
    "    flt_tbl = pd.read_parquet(partial_enc_input_dir+'/YYYYMM='+str(month))\n",
    "    tgt_enc_tbl = pd.read_parquet(enc_output_dir+'/'+set_name+'/YYYYMM='+str(month))\n",
    "    df_mg = flt_tbl.merge(tgt_enc_tbl, on=['UID'], how='left', suffixes=('_DROP', ''))\n",
    "    \n",
    "    # Drop unencoded copy of TargetEncode columns:\n",
    "    to_drop = [cc for cc in df_mg if cc.endswith('_DROP')]\n",
    "    df_mg = df_mg.drop(columns=to_drop)\n",
    "    \n",
    "    # Drop rows with missing data. Most likely due to weather fields missing.\n",
    "    df_mg = df_mg.dropna()\n",
    "    \n",
    "    # Down cast float64:\n",
    "    fp64_cols = df_mg.select_dtypes('float64').columns\n",
    "    df_mg[fp64_cols] = df_mg[fp64_cols].astype('float32')\n",
    "    \n",
    "    # All data will be read so data alignment not as important for training.\n",
    "    df_mg.to_parquet(fully_enc_output_dir+'/'+set_name+'/'+str(month)+'.parquet', flavor='spark')\n",
    "    return()\n",
    "\n",
    "\n",
    "# Use pandas and read data directly. Data is already aligned by YYYYMM.\n",
    "# dask merge adds a lot of overhead and memory usage.\n",
    "print('Merging TargetEncoder results with master flight table.')\n",
    "\n",
    "for set_name in split_labels_reorder:\n",
    "    print('Processing '+ set_name + ' set...')\n",
    "    tic = time()\n",
    "    dask.compute([merge_uid_by_month(set_name, mm) for mm in toc_df[toc_df['set_name']==set_name]['DATE'].unique()])\n",
    "    print('  took ' + str(np.round(time() - tic, 2)) + 's.')\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e525e60f-b959-4dd8-8d51-889c9072f203",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_154479/668683560.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45928ac1-03f8-4b24-8464-daef84ee4c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(fully_enc_output_dir+'/test')\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f405c2f5-07bd-45b1-bfe3-6a9edb0481cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32114cd2-12c2-4df2-929c-5ed8dbfe72a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.worker - WARNING - Heartbeat to scheduler failed\n"
     ]
    }
   ],
   "source": [
    "df.isna().sum().sort_values(ascending=False)[:30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
